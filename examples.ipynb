{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\neptune\\common\\warnings.py:62: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from Modules.train import DataModule, Model, get_trainer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "task = \"segmentation\"\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "        project=\"kaori/AISeed\",\n",
    "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyZjZiMDA2YS02MDM3LTQxZjQtOTE4YS1jODZkMTJjNGJlMDYifQ==\",\n",
    "        log_model_checkpoints=False,\n",
    "        tags=[task]\n",
    "    )\n",
    "\n",
    "with open(\"Modules/config.yaml\", 'r') as stream:\n",
    "    PARAMS = yaml.safe_load(stream)\n",
    "    PARAMS = PARAMS['segment']\n",
    "    print(PARAMS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "img_size = PARAMS['dataset_settings']['img_size']\n",
    "transforms_img = A.Compose([\n",
    "    A.RandomResizedCrop(img_size, img_size),  # Random crop and resize\n",
    "    A.HorizontalFlip(),  # Random horizontal flip\n",
    "    A.VerticalFlip(),  # Random vertical flip\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "data = DataModule(PARAMS['dataset_settings'], PARAMS['training_settings'], [transforms_img, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "neptune_logger.log_hyperparams(params=PARAMS)\n",
    "model = Model(PARAMS=PARAMS, task = task)\n",
    "trainer = get_trainer(PARAMS['training_settings'], neptune_logger)\n",
    "# \n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "import torch\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "import os\n",
    "from Modules.ultis import normalize_image\n",
    "\n",
    "with open(\"models/class_name.txt\", \"r\", encoding='utf-8') as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "model_list = [name.split('.')[0] for name in os.listdir(\"models\") if name.endswith('ckpt')]\n",
    "model_list += [\"fcn\", \"resnet\", \"fasterrcnn\", \"maskrcnn\"]\n",
    "\n",
    "def predict(image, model_choice):\n",
    "    labels, segment, detection = None, None, None\n",
    "    with open(\"Modules/config.yaml\", 'r') as stream:\n",
    "            PARAMS = yaml.safe_load(stream)\n",
    "            PARAMS = PARAMS['segment']\n",
    "    PARAMS['architect_settings']['backbone']['is_full'] = True\n",
    "    if(model_choice == \"fcn\"):\n",
    "        PARAMS['architect_settings']['backbone']['name'] = 'fcn-m'    \n",
    "        model = Model(PARAMS, task=\"segmentation\")\n",
    "    elif(model_choice == \"resnet\"):\n",
    "        PARAMS['architect_settings']['backbone']['name'] = 'resnet-s'    \n",
    "        model = Model(PARAMS, task=\"classification\")\n",
    "    elif(model_choice == \"fasterrcnn\"):\n",
    "        PARAMS['architect_settings']['backbone']['name'] = 'fasterrcnn-s'    \n",
    "        model = Model(PARAMS, task=\"detection\")\n",
    "    elif(model_choice == \"maskrcnn\"):\n",
    "        PARAMS['architect_settings']['backbone']['name'] = 'maskrcnn-s'    \n",
    "        model = Model(PARAMS, task=\"detection\")\n",
    "    else:\n",
    "        model = Model.load_from_checkpoint(f\"models/{model_choice}.ckpt\").cpu()\n",
    "    model.eval()\n",
    "    transforms = model.model.preprocess\n",
    "    tensor_image = transforms(image)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(tensor_image.unsqueeze(0))\n",
    "        if(model.task == \"classification\"):\n",
    "            preds = torch.softmax(y_hat, dim=-1).tolist()\n",
    "            labels = {class_names[k]: float(v) for k, v in enumerate(preds[0][:-1])}\n",
    "        elif(model.task == \"segmentation\"):\n",
    "            normalized_masks = torch.nn.functional.softmax(y_hat, dim=1)\n",
    "            num_classes = normalized_masks.shape[1]\n",
    "            masks = normalized_masks[0]\n",
    "            classes_masks = masks.argmax(0) == torch.arange(num_classes)[:, None, None]\n",
    "            segment = draw_segmentation_masks((tensor_image * 255.).to(torch.uint8), \n",
    "                                              masks=classes_masks, alpha=.6)\n",
    "            segment = segment.numpy().transpose(1, 2, 0) / 255.\n",
    "        elif(model.task == \"detection\"):\n",
    "            if(\"maskrcnn\" in model_choice):\n",
    "                boolean_masks = [out['masks'][out['scores'] > .75] > 0.5\n",
    "                                for out in y_hat][0]\n",
    "                detection = draw_segmentation_masks((tensor_image * 255.).to(torch.uint8),\n",
    "                                                    boolean_masks.squeeze(1), alpha=0.8)\n",
    "                detection = detection.numpy().transpose(1, 2, 0) / 255.\n",
    "            else:\n",
    "                detection = draw_bounding_boxes((tensor_image * 255.).to(torch.uint8), \n",
    "                                                    boxes=y_hat[0][\"boxes\"][:5],\n",
    "                                                    colors=\"red\",\n",
    "                                                    width=5)\n",
    "                detection = detection.numpy().transpose(1, 2, 0) / 255.\n",
    "\n",
    "    return labels, segment, detection\n",
    "\n",
    "    return labels, segment, detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\gradio\\routes.py\", line 408, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\gradio\\blocks.py\", line 1315, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\gradio\\blocks.py\", line 1043, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\anyio\\to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\197796\\AppData\\Local\\Temp\\ipykernel_1016\\3201015825.py\", line 32, in predict\n",
      "    model = Model.load_from_checkpoint(f\"models/{model_choice}.ckpt\").cpu()\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\pytorch_lightning\\core\\module.py\", line 1531, in load_from_checkpoint\n",
      "    loaded = _load_from_checkpoint(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\pytorch_lightning\\core\\saving.py\", line 60, in _load_from_checkpoint\n",
      "    checkpoint = pl_load(checkpoint_path, map_location=map_location)\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py\", line 50, in _load\n",
      "    with fs.open(path_or_url, \"rb\") as f:\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\fsspec\\spec.py\", line 1199, in open\n",
      "    f = self._open(\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 183, in _open\n",
      "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 314, in __init__\n",
      "    self._open()\n",
      "  File \"c:\\Users\\197796\\Anaconda3\\envs\\AISeed\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 319, in _open\n",
      "    self.f = open(self.path, mode=self.mode)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'e:/code/AISeed/Pytorch-pretrained-models/models/.ckpt'\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "title = \"Application Demo \"\n",
    "description = \"# A Demo of Wrapping Pretrained Networks\"\n",
    "example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    demo.title = title\n",
    "    gr.Markdown(description)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            model = gr.Dropdown(model_list, label=\"Select Model\", interactive=True)\n",
    "            im = gr.Image(type=\"pil\", label=\"input image\")\n",
    "        with gr.Column():\n",
    "            label_conv = gr.Label(label=\"Predictions\", num_top_classes=4)\n",
    "            im_segment = gr.Image(type=\"pil\", label=\"Segment\")\n",
    "            im_detection = gr.Image(type=\"pil\", label=\"Detection\")\n",
    "            btn = gr.Button(value=\"predict\")\n",
    "    btn.click(predict, inputs=[im, model], outputs=[label_conv, im_segment, im_detection])\n",
    "    gr.Examples(examples=example_list, inputs=[im, model], outputs=[label_conv, im_segment, im_detection])\n",
    "      \n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capsule",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
