{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def show_PIL_image(image):\n",
    "    if(len(image.shape) < 3):\n",
    "        image = image.unsqueeze(0)\n",
    "    print(image)\n",
    "    image = ToPILImage()(image.float())\n",
    "    plt.imshow(image)\n",
    "    plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.ultis import DubaiAerialread\n",
    "import albumentations as A\n",
    "from torchvision import transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "\n",
    "transforms_img = A.Compose([\n",
    "    # A.RandomResizedCrop(256, 256),  # Random crop and resize\n",
    "    A.HorizontalFlip(),  # Random horizontal flip\n",
    "    A.VerticalFlip(),  # Random vertical flip\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "transforms_mask = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=(256, 256)),  # Random crop and resize\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomVerticalFlip(),  # Random vertical flip\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "dataset = DubaiAerialread(data_path='data/Semantic segmentation dataset', transform=None)\n",
    "\n",
    "image, mask = dataset[10]\n",
    "print(type(mask))\n",
    "show_PIL_image(image)\n",
    "show_PIL_image(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.Detector import WrappingDetector\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "import PIL.Image as Image\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from Modules.ultis import PennFudanDataset\n",
    "import torch\n",
    "\n",
    "architect_settings = {\n",
    "                        \"name\": \"model-test\",\n",
    "                        \"backbone\": {\n",
    "                                \"name\": \"maskrcnn-s\",\n",
    "                                \"is_full\": True,\n",
    "                                \"is_pretrained\": True,\n",
    "                                \"is_freeze\": False, \n",
    "                        },\n",
    "                        \"n_cls\": 2\n",
    "                }\n",
    "\n",
    "model = WrappingDetector(architect_settings)\n",
    "model.eval()\n",
    "\n",
    "dataset = PennFudanDataset(data_path=\"data/PennFudanPed\")\n",
    "\n",
    "# img = read_image(\"examples/dog-puppy-on-garden.jpg\")\n",
    "image, mask = dataset[10]\n",
    "img_tensor = model.preprocess(image)\n",
    "\n",
    "output = model([img_tensor], [mask])\n",
    "\n",
    "score_threshold = .75\n",
    "proba_threshold = 0.5\n",
    "\n",
    "boolean_masks = [\n",
    "    out['masks'][out['scores'] > score_threshold] > proba_threshold\n",
    "    for out in output\n",
    "][0]\n",
    "\n",
    "segmet = draw_segmentation_masks((image * 255.).to(torch.uint8), boolean_masks.squeeze(), alpha=0.9)\n",
    "\n",
    "plt.imshow(segmet.permute(1, 2 , 0))\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.Segment import WrappingSegment\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "# import PIL.Image as Image\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "# from Modules.ultis import PennFudanDataset\n",
    "import torch\n",
    "\n",
    "architect_settings = {\n",
    "                        \"name\": \"model-test\",\n",
    "                        \"backbone\": {\n",
    "                                \"name\": \"fcn-m\",\n",
    "                                \"is_full\": True,\n",
    "                                \"is_pretrained\": True,\n",
    "                                \"is_freeze\": False, \n",
    "                        },\n",
    "                        \"n_cls\": 2\n",
    "                }\n",
    "\n",
    "model = WrappingSegment(architect_settings)\n",
    "model.eval()\n",
    "\n",
    "img = read_image(\"examples/Cat-or-Person-.jpg\")\n",
    "img_tensor = model.preprocess(img)\n",
    "\n",
    "output = model(img_tensor.unsqueeze(0))\n",
    "\n",
    "# normalized_masks = torch.nn.functional.softmax(output, dim=1)\n",
    "num_classes = output.shape[1]\n",
    "dog1_masks = output[0]\n",
    "dog1_all_classes_masks = dog1_masks.argmax(0) == torch.arange(num_classes)[:, None, None]\n",
    "\n",
    "segmet = draw_segmentation_masks((img * 255.).to(torch.uint8), dog1_all_classes_masks, alpha=0.5)\n",
    "\n",
    "plt.imshow(segmet.permute(1, 2 , 0))\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AISeed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
